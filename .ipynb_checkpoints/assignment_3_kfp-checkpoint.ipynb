{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFP_HOST_NAME='https://2401d996fdc4badc-dot-us-central2.pipelines.googleusercontent.com'\n",
    "client = kfp.Client(host=KFP_HOST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # preprocessing\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler,OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "# from sklearn.pipeline import make_pipeline,Pipeline\n",
    "# from sklearn.compose import make_column_transformer\n",
    "# from sklearn.model_selection import cross_validate,ShuffleSplit\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "\n",
    "# # models\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression, SGDRegressor, RidgeCV\n",
    "# from sklearn.svm import SVR, LinearSVR\n",
    "# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor \n",
    "# from sklearn.model_selection import cross_val_predict as cvp\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def data_preprocessing(raw_data_path: str, prep_data_path: str, test_data_path: str, bucket: str) -> str:\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler,OneHotEncoder\n",
    "    from sklearn.pipeline import make_pipeline,Pipeline\n",
    "    from sklearn.compose import make_column_transformer\n",
    "    from google.cloud import storage    \n",
    "    from io import BytesIO\n",
    "\n",
    "    # client = storage.Client()\n",
    "    # file_name = raw_data\n",
    "    # bucket = client.get_bucket(bucket_name)\n",
    "    # blob = bucket.get_blob(raw_data)\n",
    "    # content = blob.download_as_string()\n",
    "    #df = pd.read_csv(BytesIO(content))\n",
    "    df = pd.read_csv(bucket + raw_data_path)\n",
    "\n",
    "\n",
    "    #deleting unwanted columns\n",
    "    drop_columns = ['id','url', 'region', 'region_url','model','title_status', 'title_status','county', 'vin', 'description','size', 'image_url', 'lat','long','state','paint_color','cylinders']\n",
    "    df = df.drop(columns=drop_columns)\n",
    "    #deleting rows with nan values\n",
    "    df = df.dropna()\n",
    "    #reformatting/cleaning numeric columns\n",
    "    df['price'] = df['price'].astype(int)\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['odometer'] = df['odometer'].astype(int)\n",
    "    df['odometer'] = df['odometer'] // 5000\n",
    "    df = df[df['year'] > 110]\n",
    "    df = df[(df['price']>1000) & (df['price']<50000)]\n",
    "\n",
    "    #reformatting/cleaning categorical columns\n",
    "    df['manufacturer'] = df['manufacturer'].astype(str)\n",
    "    df['condition'] = df['condition'].astype(str)\n",
    "    # df['cylinders'] = df['cylinders'].astype(str)\n",
    "    df['fuel'] = df['fuel'].astype(str)\n",
    "    df['transmission'] = df['transmission'].astype(str)\n",
    "    df['drive'] = df['drive'].astype(str)\n",
    "    df['type'] = df['type'].astype(str)\n",
    "    df=df[df['transmission']!='other']\n",
    "    df=df.reset_index()\n",
    "\n",
    "    #label encode columns\n",
    "\n",
    "    lab_cat_columns=['condition','transmission']\n",
    "\n",
    "    for col in lab_cat_columns:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(list(df[col].astype(str).values))\n",
    "            df[col] = le.transform(list(df[col].astype(str).values))\n",
    "            \n",
    "    #Creating pipeline\n",
    "\n",
    "    numerical_features=['year', 'odometer']\n",
    "    one_hot_cat_columns=['manufacturer','fuel','drive','type']\n",
    "\n",
    "\n",
    "    categoric_transformer = make_pipeline(OneHotEncoder(sparse=False,handle_unknown='ignore'))\n",
    "\n",
    "    # Creating a pipeline with mean imputer for numerical data \n",
    "    numeric_transformer =  make_pipeline(StandardScaler())  \n",
    "\n",
    "    #Creating label transformer\n",
    "\n",
    "    # label_transformer=make_pipeline(LabelEncoder())\n",
    "\n",
    "    # Combining both pipelines such that each pipeline works on the columns it was meant for\n",
    "    preprocessor = make_column_transformer((categoric_transformer,one_hot_cat_columns),\n",
    "                                            (numeric_transformer,numerical_features))\n",
    "    #                                           (label_transformer,lab_cat_columns))\n",
    "\n",
    "    pipe=Pipeline(steps = [('prep',preprocessor)])\n",
    "    results=pipe.fit_transform(df)\n",
    "    results=pd.DataFrame(data=results, columns=list(pd.get_dummies(df[one_hot_cat_columns]).columns)+numerical_features )\n",
    "\n",
    "    final_df=results\n",
    "    # final_df['year']=df['year']\n",
    "    # final_df['odometer']=df['odometer']\n",
    "    final_df['condition']=df['condition']\n",
    "    final_df['transmission']=df['transmission']\n",
    "    final_df['price']=df['price']\n",
    "    \n",
    "    training_df = final_df.sample(frac=0.7,random_state= 0)\n",
    "    test_df= final_df.drop(training_df.index)\n",
    "\n",
    "    training_df.to_csv(bucket + prep_data_path)\n",
    "    test_df.to_csv(bucket + test_data_path)\n",
    "    \n",
    "    return prep_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing_op = comp.create_component_from_func(\n",
    "    data_preprocessing, output_component_file='data_preprocessing.yaml', packages_to_install=['pandas','scikit-learn', 'fsspec', 'gcsfs', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _read_and_split_data (path, bucket):\n",
    "#     #read csv file from bucket\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     df = pd.read_csv(bucket + path)\n",
    "\n",
    "#     #Seperating dataset and target variable\n",
    "#     target_name = 'price'\n",
    "#     df_target = df[target_name]\n",
    "#     df = df.drop([target_name], axis=1)\n",
    "#     #Train test split\n",
    "#     train, test, target, target_test = train_test_split(df, df_target, test_size=0.2, random_state=0)\n",
    "#     return [train, test, target, target_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_model_training(prep_data_path: str, bucket: str, bucket_name: str, lin_model_path: str) ->str:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from google.cloud import storage  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import _pickle as cPickle \n",
    "    from sklearn import metrics\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    df = pd.read_csv(bucket + prep_data_path)\n",
    "\n",
    "    #Seperating dataset and target variable\n",
    "    target_name = 'price'\n",
    "    df_target = df[target_name]\n",
    "    df = df.drop([target_name], axis=1)\n",
    "    #Train test split\n",
    "    train, test, target, target_test = train_test_split(df, df_target, test_size=0.2, random_state=0)\n",
    "    #return [train, test, target, target_test]\n",
    "    \n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(train, target)\n",
    "    score=linreg.score(train, target)\n",
    "    y_pred=linreg.predict(test)\n",
    "    # # values_predictions = pd.DataFrame({'Actual': target_test, 'Predicted': predictions})\n",
    "    # print('Mean Absolute Error:', metrics.mean_absolute_error(target_test, y_pred))\n",
    "    # print('Mean Squared Error:', metrics.mean_squared_error(target_test, y_pred))\n",
    "    # print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(target_test, y_pred)))\n",
    "    print('R^2 on the train set', score)\n",
    "    print('R^2 on the test set', metrics.r2_score(target_test, y_pred))\n",
    "    \n",
    "    temp_model_path='/tmp/lin_model.pickle'\n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        cPickle.dump(linreg, f, -1)\n",
    "    \n",
    "    # parse = urlparse(url=tuned_model_path, allow_fragments=False)\n",
    "    \n",
    "    # if parse.path[0] =='/':\n",
    "    #     model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    model = bucket.blob(lin_model_path)\n",
    "    model.upload_from_filename(temp_model_path)\n",
    "    return lin_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model_training_op = comp.create_component_from_func(\n",
    "    lin_model_training, output_component_file='lin_model_training.yaml', packages_to_install=['pandas','scikit-learn', 'fsspec', 'gcsfs', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model_training(prep_data_path: str, bucket: str, bucket_name: str, xgb_model_path: str) ->str:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from google.cloud import storage  \n",
    "    from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV, StratifiedKFold\n",
    "    import _pickle as cPickle \n",
    "    from sklearn import metrics\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    df = pd.read_csv(bucket + prep_data_path)\n",
    "\n",
    "    #Seperating dataset and target variable\n",
    "    target_name = 'price'\n",
    "    df_target = df[target_name]\n",
    "    df = df.drop([target_name], axis=1)\n",
    "    #Train test split\n",
    "    train, test, target, target_test = train_test_split(df, df_target, test_size=0.2, random_state=0)\n",
    "    \n",
    "    #{'objective': 'reg:squarederror'}\n",
    "    xgb_clf = xgb.XGBRegressor() \n",
    "    parameters = {'n_estimators': [60, 100, 120, 140], \n",
    "              'learning_rate': [0.01, 0.1],\n",
    "              'max_depth': [5, 7],\n",
    "              'reg_lambda': [0.5]}\n",
    "    xgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=5, n_jobs=-1).fit(train, target)\n",
    "    print(\"Best score: %0.3f\" % xgb_reg.best_score_)\n",
    "    print(\"Best parameters set:\", xgb_reg.best_params_)\n",
    "    \n",
    "    \n",
    "    ypred = xgb_reg.predict(test)\n",
    "    print('R2 score', metrics.r2_score(target_test, ypred))\n",
    "    \n",
    "    temp_model_path='/tmp/xgb_model.pickle'\n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        cPickle.dump(xgb_reg, f, -1)\n",
    "    \n",
    "    # parse = urlparse(url=tuned_model_path, allow_fragments=False)\n",
    "    \n",
    "    # if parse.path[0] =='/':\n",
    "    #     model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    model = bucket.blob(xgb_model_path)\n",
    "    model.upload_from_filename(temp_model_path)\n",
    "    return xgb_model_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_training_op = comp.create_component_from_func(\n",
    "    xgb_model_training, output_component_file='xgb_model_training.yaml', packages_to_install=['pandas','scikit-learn', 'xgboost', 'fsspec', 'gcsfs', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model_training(prep_data_path: str, bucket: str, bucket_name: str, rf_model_path: str) ->str:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from google.cloud import storage  \n",
    "    from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV, StratifiedKFold\n",
    "    import _pickle as cPickle \n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import RandomForestRegressor  \n",
    "\n",
    "    df = pd.read_csv(bucket + prep_data_path)\n",
    "\n",
    "    #Seperating dataset and target variable\n",
    "    target_name = 'price'\n",
    "    df_target = df[target_name]\n",
    "    df = df.drop([target_name], axis=1)\n",
    "    #Train test split\n",
    "    train, test, target, target_test = train_test_split(df, df_target, test_size=0.2, random_state=0)\n",
    "    #return [train, test, target, target_test]\n",
    "\n",
    "    #read preprocessed data\n",
    "    #train, test, target, target_test = _read_and_split_data(prep_data_path, bucket)\n",
    "    #Tuning RF Parameters\n",
    "    rf_param_grid = {'n_estimators': [100, 300, 500],\n",
    "                'max_features': [0.5, 0.8]\n",
    "                }\n",
    "    rf_GS = GridSearchCV(RandomForestRegressor(n_jobs=-1), param_grid=rf_param_grid,\n",
    "                    cv=ShuffleSplit(n_splits=3,random_state=1), verbose=False, pre_dispatch='2*n_jobs')\n",
    "\n",
    "    rf_GS.fit(train, target)\n",
    "\n",
    "\n",
    "    score=rf_GS.score(train, target)\n",
    "    y_pred=rf_GS.predict(test)\n",
    "    #print('R^2 on the train set', score)\n",
    "    print('R2 score', metrics.r2_score(target_test, y_pred))\n",
    "\n",
    "    temp_model_path='/tmp/rf_model.pickle'\n",
    "    with open(temp_model_path, 'wb') as f:\n",
    "        cPickle.dump(rf_GS, f, -1)\n",
    "    \n",
    "    # parse = urlparse(url=tuned_model_path, allow_fragments=False)\n",
    "    \n",
    "    # if parse.path[0] =='/':\n",
    "    #     model_path = parse.path[1:]\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    model = bucket.blob(rf_model_path)\n",
    "    model.upload_from_filename(temp_model_path)\n",
    "    return rf_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_training_op = comp.create_component_from_func(\n",
    "    rf_model_training, output_component_file='rf_model_training.yaml', packages_to_install=['pandas','scikit-learn', 'fsspec', 'gcsfs', 'google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{pipelineparam:op=;name=raw_data_path}} {{pipelineparam:op=;name=prep_data_path}} {{pipelineparam:op=;name=bucket}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Data preprocessing() missing 1 required positional argument: 'bucket'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-8260c41e8cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Create a pipeline run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_from_pipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_car_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mcreate_run_from_pipeline_func\u001b[0;34m(self, pipeline_func, arguments, run_name, experiment_name, pipeline_conf, namespace)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m       \u001b[0mpipeline_package_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pipeline.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m       \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_run_from_pipeline_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_package_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mpipeline_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m           \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m           package_path=package_path)\n\u001b[0m\u001b[1;32m    924\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check_old_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mpipeline_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mparams_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         pipeline_conf)\n\u001b[0m\u001b[1;32m    978\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0m_validate_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m       \u001b[0mpipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0mpipeline_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_conf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;31m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-8260c41e8cd4>\u001b[0m in \u001b[0;36mused_car_pipeline\u001b[0;34m(raw_data_path, prep_data_path, bucket, bucket_name, lin_model_path, rf_model_path, xgb_model_path, disable_cache)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprep_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata_preprocessing_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprep_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlin_model_training_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin_model_training_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_preprocessing_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrf_model_training_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model_training_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_preprocessing_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Data preprocessing() missing 1 required positional argument: 'bucket'"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Used car value',\n",
    "  description='estimating the price of used cars'\n",
    ")\n",
    "\n",
    "def used_car_pipeline(raw_data_path, prep_data_path, test_data_path, bucket, bucket_name, lin_model_path, rf_model_path, xgb_model_path, disable_cache): \n",
    "    \n",
    "    print(raw_data_path, prep_data_path, bucket)\n",
    "    data_preprocessing_task = data_preprocessing_op(raw_data_path, prep_data_path, test_data_path, bucket)\n",
    "    lin_model_training_task = lin_model_training_op(data_preprocessing_task.output, bucket, bucket_name, lin_model_path)\n",
    "    rf_model_training_task = rf_model_training_op(data_preprocessing_task.output, bucket, bucket_name, rf_model_path)\n",
    "    xgb_model_training_task = xgb_model_training_op(data_preprocessing_task.output, bucket, bucket_name, xgb_model_path)\n",
    "\n",
    "    if disable_cache:\n",
    "        data_preprocessing_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        rf_model_training_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "arguments = {\n",
    "    'raw_data_path': '/data/raw_vehicles.csv',\n",
    "    'prep_data_path': '/data/prep_vehicles.csv',\n",
    "    'test_data_path': '/data/test_prep_vehicles.csv',\n",
    "    'bucket': 'gs://de-3',\n",
    "    'bucket_name': 'de-3',\n",
    "    'lin_model_path': 'models/lin_model.pickle',\n",
    "    'rf_model_path': 'models/rf_model.pickle',\n",
    "    'xgb_model_path': 'models/xgb_model.pickle',\n",
    "    'disable_cache': False\n",
    "}\n",
    "\n",
    "# Create a pipeline run\n",
    "client.create_run_from_pipeline_func(used_car_pipeline, arguments= arguments)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
